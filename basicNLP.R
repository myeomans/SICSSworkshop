
######### Simple bag of words

testDox<-c("this is a test sentence.", 
           "I am providing another sentence to test.",
           "this is a test document. It has two sentences")

# Default settings - 1-grams, stemmed words
ngramTokens(testDox,ngrams=1)

# 1-grams, no stemming (notice sentences and sentence in separate columns now)
ngramTokens(testDox,ngrams=1,wstem="none")

# removing stop words this time (there's a lot of them!)
ngramTokens(testDox,ngrams=1,stop.words=FALSE)

# now let's grab 3-grams - three-word sequences.
ngramTokens(testDox,ngrams=3)

# Finally, let's grab n-grams - all 1-, 2-, and 3-word sequences.
ngramTokens(testDox,ngrams=1:3)

######### New data - restaurant reviews

# There are way too many examples for us... Let's trim it down
# This is the only city I've lived in from the data (no London or Toronto, alas!)
# bus_small<-busses %>%
#   filter(city=="Cambridge")
# rev_small <- reviews %>%
#   filter(business_id%in%bus_small$business_id)

# Save data as we go to save time (always)
# saveRDS(rev_small,file="data/rev_small.RDS")
# saveRDS(bus_small,file="data/bus_small.RDS")

# Let's load it from memory
rev_small<-readRDS("data/rev_small.RDS")
bus_small<-readRDS("data/bus_small.RDS")

# Get rid of the big dataset for now, to save resources
#rm(busses,reviews)

# The weird value in the second argument is called a "regular expression"
# Here it's just counting consecutive sets of letters i.e. words
rev_small$word_count<-str_count(rev_small$text,"[[:alpha:]]+")

# Distribution of word counts
rev_small %>%
  ggplot(aes(x=word_count)) +
  geom_histogram()


# Calculate a 1-gram feature count matrix for the review data
dfm<-ngramTokens(rev_small$text,ngrams=1)

dim(dfm)

# most common words - obvious
sort(colMeans(dfm),decreasing=TRUE)[1:20]

# least common words (note - ngramTokens filters words that occur in less than 0.5% of documents)
sort(colMeans(dfm))[1:20]

# If you turn off filtering of rare words...
dfmAll<-ngramTokens(rev_small$text[1:1000],ngrams=1,sparse = 1)

# many words only occur once! Not very useful....
sort(colSums(dfmAll)[colSums(dfmAll)==1])

# don't do this. let's get rid of rare words
rm(dfmAll)

######## Ok, let's build a model to predict sentiment!

# Let's use 1-, 2- and 3-grams
dfm<-ngramTokens(rev_small$text,ngrams=1:3)

dim(dfm)

# First, randomly split into training and testing data
train_split=sample(1:nrow(rev_small),round(nrow(rev_small)/2))

# Put training data into LASSO model

mod<-glmnet::cv.glmnet(x=dfm[train_split,],
                       y=rev_small$stars[train_split],
                       parallel=T)

# Did our model beat baseline? yes! big dip in out-of sample error
# Also note counts along top of graph - our model uses hundreds of features
plot(mod)


################ Evaluating accuracy of the model

# fit the trained model to the test data
test_predict<-predict(mod,newx = dfm[-train_split,])

# Distribution of predictions seems reasonable (some out-of-range extremes, though)
hist(test_predict)

#grab the true star ratings from 
test_actual<-rev_small$stars[-train_split]


# Accuracy using pearson correlation... not bad!
cor.test(test_predict,test_actual)

# compared to what? Let's benchmark against some others

# basic word list annotated out of context by mTurk workers
sentiment_one<-syuzhet::get_sentiment(rev_small$text[-train_split],method="nrc")

# okay... not great
cor.test(sentiment_one,test_actual)

# word list from an ML model trained on movie reviews
sentiment_two<-syuzhet::get_sentiment(rev_small$text[-train_split],method="bing")

# better!
cor.test(sentiment_two,test_actual)


# uses sentence boundaries and handles negations... a bit smarter
sentiment_three<-sentimentr::sentiment(rev_small$text[-train_split]) %>%
  group_by(element_id) %>%
  summarize(sent=mean(sentiment))

# better still!
cor.test(sentiment_three$sent,test_actual)


############ Interpreting the model with lists and plots

# Extract coefficients of model into a table
scoreSet<-coef(mod) %>%
  as.matrix() %>%
  data.frame() %>%
  rownames_to_column(var = "ngram") %>%
  rename(score="X1") %>%
  # removes ngrams that did not get a score in the model
  filter(score!=0 & ngram!="(Intercept)" & !is.na(score)) %>%
  # this left_join line adds ngram frequencies to the table
  left_join(data.frame(ngram=colnames(dfm),
                       freq=colMeans(dfm)))

# 10 words that predict low scores
scoreSet%>%
  arrange(score) %>%
  slice(1:10)

# 10 ngrams that predict low scores
scoreSet%>%
  arrange(-score) %>%
  slice(1:10)

#combine coefficients with ngram frequencies

scoreSet %>%
  # can't plot everything... this mutate line removes some labels
  # that are not common (>1%) or distinctive enough 
  mutate(ngram=ifelse((abs(score)>.05)&(freq>.01),ngram,"")) %>%
  # let's add a bit of color
  mutate(col=case_when(
    score>.02  ~ "blue",
    score<(-.02) ~ "red",
    T ~ "black")) %>%
  ggplot(aes(x=score,y=freq,label=ngram,color=col)) +
  scale_color_manual(breaks=c("blue","black","red"),
                     values=c("blue","black","red"))+
  geom_vline(xintercept=0)+
  geom_point() +
  geom_label_repel(max.overlaps = 30,force = 6)+  
  scale_y_continuous(trans="log2",
                     breaks=c(.01,.05,.1,.2),
                     labels = paste0(c(1,5,10,20),"%"))+
  scale_x_continuous(breaks=seq(-.4,.4,.2),
                     labels = seq(-.4,.4,.2),
                     limits = c(-.4,.4))+
  theme_bw() +
  labs(x="Coefficient in Model",y="Frequency in Advice Datasets")+
  theme(legend.position = "none",
        axis.title=element_text(size=20),
        axis.text=element_text(size=16))

# Save the plot for a slide deck
ggsave("wordz.png",units="cm",dpi=200,width=35,height=20)


